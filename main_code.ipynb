{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYhB2OARw_Nq",
        "outputId": "e47f6534-2d03-4de4-cf09-fd607e053388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-community langchain-groq neo4j fastapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twENeM0RuBGL",
        "outputId": "45813e6a-0c51-495d-de1a-701fec93c64d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJwWaDtyuCug",
        "outputId": "3f92aad5-4427-452b-9ce5-0b22f5a9a5a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyUF10z3hDQr",
        "outputId": "85ab6d5a-950c-4ec3-8edd-aa5cb3535f50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama) (0.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
            "Downloading ollama-0.3.3-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import ollama\n",
        "from langchain_groq import ChatGroq\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import requests\n",
        "from fastapi import HTTPException\n",
        "\n",
        "NEO4J_URI=\"neo4j+s://efe689a5.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"tScJDhi_NBBry7rmL0rVwvkvxaEDfEL2BUK9PqgDCZY\"\n",
        "groq_api_key=\"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\"\n",
        "news_api=\"598f444492314398bf47cdf2cc525827\"\n",
        "weather_api=\"91fcdb9a0ce7eb272ea802e36dc324ae\"\n",
        "\n",
        "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
        "neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "graph = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        ")\n",
        "\n",
        "\n",
        "def fetch_weather_data(city, api_key=weather_api):\n",
        "    geocode_url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\"\n",
        "    response = requests.get(geocode_url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise HTTPException(status_code=404, detail=\"City not found\")\n",
        "\n",
        "    city_data = response.json()\n",
        "    main=city_data['weather'][0]['main']\n",
        "    description=city_data['weather'][0]['description']\n",
        "    temperature=city_data['main']['temp']\n",
        "    humidity=city_data['main']['humidity']\n",
        "    wind_speed=city_data['wind']['speed']\n",
        "    return f\"Here the weather of main {city} is {main} having {description}. Here the temperature is {temperature} with humidity {humidity} and wind speed {wind_speed}\"\n",
        "\n",
        "def fetch_news_data(city, api_key=news_api):\n",
        "    # NewsAPI endpoint for fetching news based on query (city name)\n",
        "    base_url = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "    params = {\n",
        "        'q': city,\n",
        "        'apiKey': api_key,\n",
        "        'language': 'en',  # Adjust the language as needed\n",
        "        'pageSize': 5      # Limit to 5 news articles for the response\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise HTTPException(status_code=500, detail=\"Failed to fetch news data\")\n",
        "\n",
        "    news_data = response.json()\n",
        "    articles = news_data.get('articles', [])\n",
        "\n",
        "    if not articles:\n",
        "        raise HTTPException(status_code=404, detail=\"No news articles found for the given city\")\n",
        "\n",
        "    # Extract and concatenate all content fields\n",
        "    combined_content = \" \".join([article.get('content', '') for article in articles if article.get('content')])\n",
        "\n",
        "    return combined_content\n",
        "\n",
        "# Store initial trip details\n",
        "if \"trip_initialized\" not in st.session_state:\n",
        "    st.session_state.trip_initialized = False\n",
        "\n",
        "# Get initial trip details once\n",
        "if not st.session_state.trip_initialized:\n",
        "    st.title(\"Personalized Trip Planner\")\n",
        "\n",
        "    # Initial trip information\n",
        "    st.session_state.location = st.text_input(\"Location\", \"Berlin\")\n",
        "    st.session_state.trip_date = st.date_input(\"Trip Date\")\n",
        "    st.session_state.food_pref = st.selectbox(\"Preferred Food\", [\"Local\", \"International\", \"Vegetarian\", \"Any\"])\n",
        "    st.session_state.activity_pref = st.multiselect(\"Activities\", [\"Museums\", \"Shopping\", \"Sightseeing\",\"Historical Sites\",\"All famous thing there\"])\n",
        "    st.session_state.budget = st.number_input(\"Budget in dollars\", min_value=0, value=100)\n",
        "    st.session_state.start_location = st.text_input(\"Starting Location\", \"Hotel\")\n",
        "\n",
        "    # Confirm trip details to proceed\n",
        "    if st.button(\"Confirm Trip Details\"):\n",
        "        st.session_state.trip_initialized = True\n",
        "        st.write(\"Trip details saved. You can now start the conversation.\")\n",
        "\n",
        "# Initialize conversation history\n",
        "if \"conversation_history\" not in st.session_state:\n",
        "    st.session_state.conversation_history = [\n",
        "        AIMessage(content=\"Hello! I'm your personalized trip planner assistant. Provide me a time to start your journey?\")\n",
        "    ]\n",
        "\n",
        "#extract the relevant information from the current neo4j database\n",
        "def extract_information_from_neo4j(user_query, neo4j_driver, llm=llm):\n",
        "    # Generate Cypher query to fetch relevant information based on user query and conversation history\n",
        "    # Retrieve up to the last 5 Cypher queries, depending on availability\n",
        "    if 'cypher_query_history' in st.session_state and st.session_state.cypher_query_history:\n",
        "      num_queries = min(5, len(st.session_state.cypher_query_history))\n",
        "      relevant_queries = st.session_state.cypher_query_history[-num_queries:]\n",
        "    else:\n",
        "      relevant_queries = []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Based on the user's query and relevant past Cypher queries, create a new Cypher query to extract meaningful information\n",
        "    from the Neo4j database. The Cypher query should follow a generalized format that considers user preferences, visited\n",
        "    places, recent queries, and weather data.\n",
        "\n",
        "    - **User Query**: \"{user_query}\"\n",
        "    - **Relevant Past Cypher Queries**: \"{relevant_queries}\"\n",
        "\n",
        "    The Cypher query should be similar to the example format given below without any other comment and make sure not to consider the example as fully its just for illustration how the query should look like. make use of user iput and relevant past queries to generate:\n",
        "\n",
        "    MATCH (u:User {{id: $userId}})\n",
        "    OPTIONAL MATCH (u)-[:PREFERS]->(activity:Activity)\n",
        "    OPTIONAL MATCH (u)-[:VISITED]->(place:Place)\n",
        "    OPTIONAL MATCH (u)-[:ASKED]->(query:Query)\n",
        "    OPTIONAL MATCH (weather:Weather {{location: $location, date: $date}})\n",
        "\n",
        "    WHERE\n",
        "        (activity.name IN $preferredActivities OR $preferredActivities IS NULL) AND\n",
        "        (place.budget <= $budget OR $budget IS NULL)\n",
        "\n",
        "    RETURN\n",
        "        COLLECT(DISTINCT activity.name) AS preferred_activities,\n",
        "        COLLECT(DISTINCT place.name) AS visited_places,\n",
        "        COLLECT(query.text) AS recent_queries,\n",
        "        weather.condition AS current_weather\n",
        "\n",
        "   Provide the Cypher query based on the information from the response without any other comment. This is strict that only provide cypher query not other comment\n",
        "    \"\"\"\n",
        "\n",
        "    message = HumanMessage(content=prompt)\n",
        "    # Call llm with a list containing the HumanMessage object\n",
        "    cypher_query = llm([message]).content\n",
        "\n",
        "    try:\n",
        "        # Execute the generated Cypher query on Neo4j\n",
        "        with neo4j_driver.session() as session:\n",
        "            result = session.run(cypher_query)\n",
        "            extracted_info = [record.data() for record in result]\n",
        "\n",
        "        # Check if results exist, else return an empty list\n",
        "        if not extracted_info:\n",
        "            return []\n",
        "\n",
        "        return extracted_info\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log the error (optional), and return an empty list\n",
        "        return []\n",
        "\n",
        "# Function to generate trip response\n",
        "def generate_trip_response(user_input, conversation_history, neo4j_driver=neo4j_driver, llm=llm):\n",
        "    \"\"\"\n",
        "    Generates a trip response by combining the user query, conversation history,\n",
        "    one-time collected data from session state, and extracted information from Neo4j.\n",
        "\n",
        "    Parameters:\n",
        "        user_input (str): The latest query or message from the user.\n",
        "        conversation_history (list): List of dictionaries with the conversation history.\n",
        "        extracted_info (list): Information extracted from Neo4j based on the user's context and query.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response.\n",
        "    \"\"\"\n",
        "    # Step 1: Extract relevant information from Neo4j using the user query and conversation history\n",
        "    extracted_info = extract_information_from_neo4j(user_input, neo4j_driver, llm)\n",
        "    weather_data = fetch_weather_data(st.session_state.get('location', 'Not specified'))\n",
        "    news_data = fetch_news_data(st.session_state.get('location', 'Not specified'))\n",
        "\n",
        "    # Construct the prompt using session state values for one-time data\n",
        "    prompt = f\"\"\"\n",
        "    You are a travel assistant helping the user plan their trip. Use the following details to answer the user's query:\n",
        "\n",
        "    - **Location**: {st.session_state.get('location', 'Not specified')}\n",
        "    - **Trip Date**: {st.session_state.get('trip_date', 'Not specified')}\n",
        "    - **Food Preference**: {st.session_state.get('food_pref', 'Not specified')}\n",
        "    - **Activities Preferred**: {\", \".join(st.session_state.get('activity_pref', []))}\n",
        "    - **Budget**: ${st.session_state.get('budget', 'Not specified')}\n",
        "    - **Starting Location**: {st.session_state.get('start_location', 'Not specified')}\n",
        "    - **Weather**: {weather_data}\n",
        "    - **Latest News in Location**: {news_data}\n",
        "\n",
        "    Here make sure that the news data is appropriately based on the context of the trip or else don't consider it.\n",
        "    Additional Information from User's Profile:\n",
        "    {extracted_info if extracted_info else 'No additional information available.'}\n",
        "\n",
        "    User's Conversation History:\n",
        "    {conversation_history}\n",
        "\n",
        "    Now, based on all this information, respond to the following user query:\n",
        "    \"{user_input}\"\n",
        "    Here provide more weightage to the recent and conversation history and extracted data.\n",
        "    Here is an example of how the response should be:\n",
        "    'User: Hi, I'd like to plan a one-day trip in Berlin.\n",
        "    System: Great! Let’s get started. What day are you planning for, and what time do you want\n",
        "    to start and end your day?\n",
        "    User: I'll be visiting on the 15th of June. I want to start at 8 AM and finish by 8 PM.\n",
        "    System: Noted. Could you tell me your interests? For example, do you like historical sites,\n",
        "    museums, shopping, or food experiences?\n",
        "    User: I like history and local food.\n",
        "    System: Excellent! What's your budget for the day?\n",
        "    User: I want to keep it aSordable, ideally under 100 Euros.\n",
        "    System: Perfect! Where would you like to start your day? You can provide your hotel or any\n",
        "    specific location, or we can start from the first attraction itself.\n",
        "    User: I'm staying at Hotel Berlin Central.\n",
        "    System: Got it! Here is an initial itinerary for your day in Berlin, starting from Hotel Berlin\n",
        "    Central:\n",
        "    • Start Point: Hotel Berlin Central to Brandenburg Gate (8:00 AM - 9:00 AM)\n",
        "    o Travel Method: Walk (15 minutes)\n",
        "    o Activity: Visit the iconic Brandenburg Gate and learn about its historical\n",
        "    significance.\n",
        "    • Second Stop: Holocaust Memorial (9:15 AM - 10:00 AM)\n",
        "    o Activity: Walk through the memorial dedicated to the victims of the\n",
        "    Holocaust.\n",
        "    • Third Stop: Reichstag Building (10:15 AM - 11:30 AM)\n",
        "    o Activity: Visit the Reichstag Building and, if you’d like, head up to the glass\n",
        "    dome for panoramic views of the city.\n",
        "    o Note: Pre-booking required for the dome visit (free).\n",
        "    • Lunch: Curry 36 (12:00 PM - 12:45 PM)\n",
        "    o Cuisine: Enjoy Berlin's famous Currywurst.\n",
        "    o Cost: Approx. 7 Euros\n",
        "    • Fourth Stop: Museum Island (1:15 PM - 3:00 PM)\n",
        "    o Activity: Visit Pergamon Museum or Neues Museum. Entry fees around 12-15\n",
        "    Euros per museum.\n",
        "    • Fifth Stop: Berliner Dom (3:15 PM - 4:00 PM)\n",
        "    o Activity: Explore the Berlin Cathedral, enjoy the architecture, and climb to\n",
        "    the top for a beautiful view.\n",
        "    • CoBee Break: Local Café near Alexanderplatz (4:15 PM - 4:45 PM)\n",
        "    o Cost: Approx. 5 Euros\n",
        "    • Sixth Stop: East Side Gallery (5:15 PM - 6:15 PM)\n",
        "    o Activity: Walk along the East Side Gallery, the longest remaining section of\n",
        "    the Berlin Wall, covered in murals by artists from around the world.\n",
        "    • Dinner: Prater Garten (6:45 PM - 7:45 PM)\n",
        "    o Cuisine: Traditional German beer garden experience.\n",
        "    o Cost: Approx. 20 Euros\n",
        "    System: Here is the updated itinerary with costs and optimized travel:\n",
        "    1. Start Point: Hotel Berlin Central to Brandenburg Gate (8:00 AM - 9:00 AM)\n",
        "    o Travel Method: Walk (15 minutes)\n",
        "    2. Second Stop: Holocaust Memorial (9:15 AM - 10:00 AM)\n",
        "    3. Third Stop: Reichstag Building (10:15 AM - 11:30 AM)\n",
        "    4. Lunch: Curry 36 (12:00 PM - 12:45 PM)\n",
        "    5. Fourth Stop: Museum Island (1:15 PM - 3:00 PM)\n",
        "    6. Fifth Stop: Berliner Dom (3:15 PM - 4:00 PM)\n",
        "    7. CoBee Break: Local Café near Alexanderplatz (4:15 PM - 4:45 PM)\n",
        "    8. Sixth Stop: East Side Gallery (5:15 PM - 6:15 PM)\n",
        "    9. Dinner: Prater Garten (6:45 PM - 7:45 PM)\n",
        "    Total Estimated Cost: Approx. 59 Euros\n",
        "    System: The weather forecast for the 15th of June looks sunny with mild temperatures,\n",
        "    perfect for walking tours. I recommend wearing comfortable shoes and carrying a water\n",
        "    bottle.'\n",
        "    Here consider the budget we have in dollars every time and give the budget distribution during trip planning.\n",
        "    Also for any other user input apart from the trip planner provide them a response appropriately.\n",
        "    Provide a detailed response considering all preferences, and if possible, suggest personalized itinerary items based on extracted information and past preferences.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare the message format for Ollama\n",
        "    #messages = conversation_history + [{\"role\": \"user\", \"content\": prompt}\n",
        "\n",
        "    message = AIMessage(content=prompt)\n",
        "    llm_response = llm([message]).content # Call llm with a list containing the HumanMessage object\n",
        "    return llm_response\n",
        "\n",
        "\n",
        "# Function to generate Cypher query using ChatGroq\n",
        "def generate_cypher_query(llm_response, llm=llm):\n",
        "    user_id = st.session_state.get('session_id', 'default_user')\n",
        "    prompt = f\"\"\"\n",
        "    You are a graph database assistant. The user response is as follows: \"{llm_response}\".\n",
        "    Extract meaningful information from this response as Entity-Relationship-Entity triplets in the format:\n",
        "    (User, RELATIONSHIP, Detail). Then convert this into a Cypher query that updates the Neo4j database.\n",
        "\n",
        "    Example Triplet: (\"User\", \"PLANS_TO_VISIT\", \"Berlin\")\n",
        "\n",
        "    Format the Cypher query like:\n",
        "    MERGE (a:User {{name: '{user_id}'}})\n",
        "    MERGE (b:Detail {{name: 'Extracted detail'}})\n",
        "    MERGE (a)-[:RELATIONSHIP]->(b)\n",
        "\n",
        "    Provide the Cypher query based on the information from the response without any other comment.\n",
        "    \"\"\"\n",
        "    # Create a HumanMessage object with the prompt\n",
        "    message = HumanMessage(content=prompt)\n",
        "    cypher_query = llm([message]).content # Call llm with a list containing the HumanMessage object\n",
        "    return cypher_query\n",
        "\n",
        "# Store response in Neo4j\n",
        "def store_memory_in_neo4j(cypher_query, neo4j_driver=neo4j_driver):\n",
        "    with neo4j_driver.session() as session:\n",
        "        session.run(cypher_query)\n",
        "\n",
        "# Display chat history\n",
        "for message in st.session_state.conversation_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.markdown(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.markdown(message.content)\n",
        "\n",
        "# User input for new query\n",
        "user_query = st.chat_input(\"Type a message...\")\n",
        "if user_query is not None and user_query.strip() != \"\":\n",
        "    st.session_state.conversation_history.append(HumanMessage(content=user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "        try:\n",
        "            response = generate_trip_response(user_query, st.session_state.conversation_history)\n",
        "            st.markdown(response)\n",
        "            st.session_state.conversation_history.append(AIMessage(content=response))\n",
        "            # Generate Cypher query to update Neo4j\n",
        "            cypher_query = generate_cypher_query(response)\n",
        "            store_memory_in_neo4j(cypher_query, neo4j_driver)\n",
        "            # Append the generated Cypher query to cypher_query_history for later retrieval\n",
        "            if 'cypher_query_history' not in st.session_state:\n",
        "                st.session_state.cypher_query_history = []\n",
        "            st.session_state.cypher_query_history.append(cypher_query)\n",
        "\n",
        "        except Exception as e:\n",
        "            #st.error(f\"Error processing query: {e}\")\n",
        "            pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6V2IT7T30PH",
        "outputId": "1953d934-0c46-4fcb-d36f-c69e02ab09ff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & !npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHk4V0PPuLej",
        "outputId": "88a5cfba-f9c0-4d5f-b5db-6ea696acf620"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: !npx: command not found\n",
            "34.90.227.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ownj0KYcuPPr",
        "outputId": "f1798c59-32e4-4b43-b8a9-b7a4db0fefe2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://yummy-rabbits-mate.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}